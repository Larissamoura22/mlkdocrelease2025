{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097e9ee-ab76-43e2-97f1-8d4bc0028e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def setup_download_folder(folder_name=\"mlk_pdfs\"):\n",
    "    \"\"\"\n",
    "    Creates the specified download folder if it doesn't exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        try:\n",
    "            os.makedirs(folder_name)\n",
    "            print(f\"SUCCESS: Created download folder: '{folder_name}'\")\n",
    "        except OSError as e:\n",
    "            print(f\"ERROR: Could not create folder '{folder_name}': {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"INFO: Folder '{folder_name}' already exists.\")\n",
    "    \n",
    "    # Verify if it's actually a directory\n",
    "    if os.path.isdir(folder_name):\n",
    "        print(f\"SUCCESS: '{folder_name}' is a valid directory.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"ERROR: '{folder_name}' exists but is not a directory.\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the name of the folder where PDFs will be stored\n",
    "    pdf_download_folder = \"mlk_pdfs\"\n",
    "    \n",
    "    # Run the setup function\n",
    "    folder_ready = setup_download_folder(pdf_download_folder)\n",
    "\n",
    "    if folder_ready:\n",
    "        print(\"\\nFolder setup complete. You can proceed to the next part.\")\n",
    "    else:\n",
    "        print(\"\\nFolder setup failed. Please resolve the errors before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331b4d4-a237-41af-b6c2-4398e06e1820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os # Keep os imported for potential future use or context\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the HTML content of a given URL.\n",
    "    Returns the content as a string if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"INFO: Attempting to fetch content from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10) # Added a timeout for robustness\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        print(f\"SUCCESS: Successfully fetched content from {url}\")\n",
    "        # Return a snippet of the content to confirm it's not empty, but don't print all\n",
    "        print(f\"DEBUG: First 200 characters of page content: {response.text[:200]}...\")\n",
    "        return response.text\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(f\"ERROR: HTTP Error occurred: {errh}\")\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(f\"ERROR: Connection Error occurred: {errc}\")\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(f\"ERROR: Timeout Error occurred: {errt}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"ERROR: An unexpected Request Error occurred: {err}\")\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # The URL for the MLK assassination records page\n",
    "    mlk_records_url = \"https://www.archives.gov/research/mlk\"\n",
    "    \n",
    "    # Fetch the content\n",
    "    page_html_content = fetch_page_content(mlk_records_url)\n",
    "\n",
    "    if page_html_content:\n",
    "        print(\"\\nWeb page content successfully retrieved. You can proceed to the next part.\")\n",
    "    else:\n",
    "        print(\"\\nFailed to retrieve web page content. Please review the error messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982aaffc-99b6-480c-b86f-504e58182066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse # urlparse is useful for domain checking\n",
    "import os # Just in case for path context, though not directly used in this chunk\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the HTML content of a given URL.\n",
    "    (This function is repeated here for self-containment,\n",
    "    but in a final script, it would be combined with the main logic.)\n",
    "    \"\"\"\n",
    "    print(f\"INFO: Attempting to fetch content from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        print(f\"SUCCESS: Successfully fetched content from {url}\")\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"ERROR: Failed to fetch content from {url}: {err}\")\n",
    "    return None\n",
    "\n",
    "def extract_pdf_links(html_content, base_url, limit=100):\n",
    "    \"\"\"\n",
    "    Parses HTML content to find and extract unique PDF links.\n",
    "    Filters to keep links primarily from the same domain.\n",
    "    \"\"\"\n",
    "    if not html_content:\n",
    "        print(\"ERROR: No HTML content provided to extract links from.\")\n",
    "        return set()\n",
    "\n",
    "    print(\"INFO: Starting to parse HTML for PDF links...\")\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    pdf_links = set() # Use a set to store unique URLs\n",
    "\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "    base_domain = parsed_base_url.netloc\n",
    "\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        full_url = urljoin(base_url, href)\n",
    "\n",
    "        # Basic check: Is it a PDF link?\n",
    "        if full_url.lower().endswith('.pdf'):\n",
    "            # More robust check: Is it from the same domain or a sub-domain?\n",
    "            # This helps prevent downloading PDFs from external sites linked on the page.\n",
    "            parsed_full_url = urlparse(full_url)\n",
    "            if parsed_full_url.netloc == base_domain or parsed_full_url.netloc.endswith(f\".{base_domain}\"):\n",
    "                pdf_links.add(full_url)\n",
    "                if len(pdf_links) >= limit:\n",
    "                    print(f\"INFO: Reached limit of {limit} unique PDF links during parsing. Stopping link collection.\")\n",
    "                    break\n",
    "    \n",
    "    print(f\"SUCCESS: Found {len(pdf_links)} unique PDF links.\")\n",
    "    return pdf_links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlk_records_url = \"https://www.archives.gov/research/mlk\"\n",
    "    \n",
    "    # First, fetch the content\n",
    "    page_html = fetch_page_content(mlk_records_url)\n",
    "\n",
    "    if page_html:\n",
    "        # Then, extract the PDF links\n",
    "        found_pdf_urls = extract_pdf_links(page_html, mlk_records_url, limit=100)\n",
    "        \n",
    "        print(\"\\n--- Summary of Found PDF Links (First 5) ---\")\n",
    "        if found_pdf_urls:\n",
    "            for i, link in enumerate(list(found_pdf_urls)[:5]):\n",
    "                print(f\"{i+1}. {link}\")\n",
    "            if len(found_pdf_urls) > 5:\n",
    "                print(f\"...and {len(found_pdf_urls) - 5} more.\")\n",
    "        else:\n",
    "            print(\"No PDF links found on the page.\")\n",
    "        \n",
    "        print(\"\\nPDF link extraction complete. You can proceed to the next part (downloading).\")\n",
    "    else:\n",
    "        print(\"\\nCannot extract links without page content. Please review previous errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c88cef-59aa-4f6b-971c-6ed0c3f4cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# --- Function from Part 1: Folder Setup ---\n",
    "def setup_download_folder(folder_name=\"mlk_pdfs\"):\n",
    "    \"\"\"\n",
    "    Creates the specified download folder if it doesn't exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        try:\n",
    "            os.makedirs(folder_name)\n",
    "            print(f\"SUCCESS: Created download folder: '{folder_name}'\")\n",
    "        except OSError as e:\n",
    "            print(f\"ERROR: Could not create folder '{folder_name}': {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"INFO: Folder '{folder_name}' already exists.\")\n",
    "    \n",
    "    if os.path.isdir(folder_name):\n",
    "        print(f\"SUCCESS: '{folder_name}' is a valid directory.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"ERROR: '{folder_name}' exists but is not a directory.\")\n",
    "        return False\n",
    "\n",
    "# --- Function from Part 2: Fetch Web Page Content ---\n",
    "def fetch_page_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the HTML content of a given URL.\n",
    "    Returns the content as a string if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"INFO: Attempting to fetch content from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        print(f\"SUCCESS: Successfully fetched content from {url}\")\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"ERROR: Failed to fetch content from {url}: {err}\")\n",
    "    return None\n",
    "\n",
    "# --- Function from Part 3: Extract PDF Links ---\n",
    "def extract_pdf_links(html_content, base_url, limit=100):\n",
    "    \"\"\"\n",
    "    Parses HTML content to find and extract unique PDF links.\n",
    "    Filters to keep links primarily from the same domain.\n",
    "    \"\"\"\n",
    "    if not html_content:\n",
    "        print(\"ERROR: No HTML content provided to extract links from.\")\n",
    "        return set()\n",
    "\n",
    "    print(\"INFO: Starting to parse HTML for PDF links...\")\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    pdf_links = set()\n",
    "\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "    base_domain = parsed_base_url.netloc\n",
    "\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        full_url = urljoin(base_url, href)\n",
    "\n",
    "        if full_url.lower().endswith('.pdf'):\n",
    "            parsed_full_url = urlparse(full_url)\n",
    "            if parsed_full_url.netloc == base_domain or parsed_full_url.netloc.endswith(f\".{base_domain}\"):\n",
    "                pdf_links.add(full_url)\n",
    "                if len(pdf_links) >= limit:\n",
    "                    print(f\"INFO: Reached limit of {limit} unique PDF links during parsing. Stopping link collection.\")\n",
    "                    break\n",
    "    \n",
    "    print(f\"SUCCESS: Found {len(pdf_links)} unique PDF links.\")\n",
    "    return pdf_links\n",
    "\n",
    "# --- NEW Function for Part 4: Download PDFs ---\n",
    "def download_pdfs(pdf_urls, download_folder, limit=100):\n",
    "    \"\"\"\n",
    "    Downloads PDF files from a list of URLs into the specified folder.\n",
    "    \"\"\"\n",
    "    if not pdf_urls:\n",
    "        print(\"INFO: No PDF URLs provided for download.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nINFO: Starting to download {min(len(pdf_urls), limit)} PDFs...\")\n",
    "    downloaded_count = 0\n",
    "\n",
    "    # Ensure we only try to download up to the limit\n",
    "    for pdf_url in list(pdf_urls)[:limit]:\n",
    "        if downloaded_count >= limit:\n",
    "            break # Just in case, double-check limit\n",
    "\n",
    "        file_name = os.path.basename(urlparse(pdf_url).path)\n",
    "        # Clean up file name (e.g., remove query parameters if any)\n",
    "        file_name = file_name.split('?')[0]\n",
    "        if not file_name: # Fallback if filename is empty after split\n",
    "            file_name = \"downloaded_pdf_\" + str(downloaded_count + 1) + \".pdf\"\n",
    "        \n",
    "        save_path = os.path.join(download_folder, file_name)\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"INFO: Skipping existing file: {file_name}\")\n",
    "            downloaded_count += 1 # Count it as 'handled' even if skipped\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Downloading {downloaded_count + 1}/{limit}: {file_name} from {pdf_url}\")\n",
    "            pdf_response = requests.get(pdf_url, stream=True, timeout=30) # Increased timeout for large files\n",
    "            pdf_response.raise_for_status()\n",
    "\n",
    "            with open(save_path, 'wb') as f:\n",
    "                for chunk in pdf_response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"SUCCESS: Downloaded: {file_name}\")\n",
    "            downloaded_count += 1\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"ERROR: Failed to download {pdf_url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: An unexpected error occurred while processing {pdf_url}: {e}\")\n",
    "\n",
    "    print(f\"\\nFinished download process. Total PDFs downloaded/skipped: {downloaded_count}/{min(len(pdf_urls), limit)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlk_records_url = \"https://www.archives.gov/research/mlk\"\n",
    "    pdf_download_folder = \"mlk_pdfs\" # This is the folder created in Part 1\n",
    "    download_limit = 100 # Adjust this if you want more or fewer than 100 PDFs\n",
    "\n",
    "    # --- Step 1: Setup Folder ---\n",
    "    folder_is_ready = setup_download_folder(pdf_download_folder)\n",
    "    if not folder_is_ready:\n",
    "        print(\"Aborting download process due to folder issues.\")\n",
    "        exit() # Stop if folder setup failed\n",
    "\n",
    "    # --- Step 2: Fetch Page Content ---\n",
    "    page_html = fetch_page_content(mlk_records_url)\n",
    "    if not page_html:\n",
    "        print(\"Aborting download process as page content could not be retrieved.\")\n",
    "        exit() # Stop if page content fetch failed\n",
    "\n",
    "    # --- Step 3: Extract PDF Links ---\n",
    "    found_pdf_urls = extract_pdf_links(page_html, mlk_records_url, limit=download_limit)\n",
    "    if not found_pdf_urls:\n",
    "        print(\"No PDF links found. Aborting download process.\")\n",
    "        exit() # Stop if no links found\n",
    "\n",
    "    # --- Step 4: Download PDFs ---\n",
    "    download_pdfs(found_pdf_urls, pdf_download_folder, limit=download_limit)\n",
    "\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"PDF download script finished. Check your 'mlk_pdfs' folder.\")\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2ce63-28b0-44ab-8a66-53fdc841391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pypdf import PdfWriter # Corrected: Using PdfWriter as PdfMerger is deprecated\n",
    "\n",
    "def combine_pdfs_in_folder(input_folder, output_filename=\"merged_mlk_records.pdf\"):\n",
    "    \"\"\"\n",
    "    Combines all PDF files in a given folder into a single PDF.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): The path to the folder containing the PDF files.\n",
    "        output_filename (str): The name of the output merged PDF file.\n",
    "    \"\"\"\n",
    "    merger = PdfWriter() # Corrected: Initializing PdfWriter\n",
    "    pdf_files = []\n",
    "\n",
    "    # Collect all PDF files in the input folder\n",
    "    print(f\"INFO: Scanning '{input_folder}' for PDF files...\")\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            # Ensure we're only adding PDFs and not the output file itself if it already exists\n",
    "            if file.lower().endswith('.pdf') and file.lower() != output_filename.lower():\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Sort files to ensure consistent merging order (e.g., by name)\n",
    "    pdf_files.sort()\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"INFO: No PDF files found in '{input_folder}' to merge. (Make sure your downloaded PDFs are in there!)\")\n",
    "        return\n",
    "\n",
    "    print(f\"INFO: Found {len(pdf_files)} PDFs to merge. Starting merge process...\")\n",
    "\n",
    "    for pdf in pdf_files:\n",
    "        try:\n",
    "            merger.append(pdf)\n",
    "            print(f\"INFO: Appended: {os.path.basename(pdf)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not append {os.path.basename(pdf)}: {e}\")\n",
    "\n",
    "    try:\n",
    "        output_path = os.path.join(input_folder, output_filename)\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            merger.write(f)\n",
    "        print(f\"\\nSUCCESS: All PDFs merged successfully into: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error writing merged PDF: {e}\")\n",
    "    finally:\n",
    "        merger.close() # Ensure all file handles are closed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_input_folder = \"mlk_pdfs\" # This should be the folder where your downloaded PDFs are\n",
    "    combine_pdfs_in_folder(pdf_input_folder)\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"PDF merging script finished. Check your 'mlk_pdfs' folder for the merged file.\")\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f7c99-f52e-450c-906c-4746f65cc62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72520c95-5019-4402-8896-270829c6e3da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ocrmypdf\n",
    "import os\n",
    "\n",
    "def ocr_pdf_file(input_pdf_path, output_pdf_path, language='eng'):\n",
    "    \"\"\"\n",
    "    Performs OCR on a PDF file and creates a new searchable PDF.\n",
    "    Updated for ocrmypdf v16.x.x API.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_pdf_path):\n",
    "        print(f\"ERROR: Input PDF not found at {input_pdf_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"INFO: Starting OCR for {os.path.basename(input_pdf_path)}...\")\n",
    "    print(f\"This may take a while, especially for a large file (100 PDFs merged).\")\n",
    "    try:\n",
    "        ocrmypdf.ocr(\n",
    "            input_file=input_pdf_path,\n",
    "            output_file=output_pdf_path,\n",
    "            language=language,\n",
    "            force_ocr=True,\n",
    "            optimize=1,\n",
    "        )\n",
    "        print(f\"\\nSUCCESS: OCR completed successfully! Searchable PDF saved to: {output_pdf_path}\")\n",
    "    except ocrmypdf.exceptions.InputFileError as e:\n",
    "        print(f\"ERROR: OCR error: Input file issue - {e}\")\n",
    "    except Exception as e:\n",
    "        error_message = str(e).lower()\n",
    "        if \"tesseract\" in error_message or \"ghostscript\" in error_message or \"command not found\" in error_message:\n",
    "            print(f\"ERROR: OCR error: Tesseract or Ghostscript command failed. This is often due to them not being installed or not in your system's PATH. Please verify their installation. Details: {e}\")\n",
    "        else:\n",
    "            print(f\"ERROR: An unexpected OCR error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_folder = \"mlk_pdfs\"\n",
    "    input_merged_pdf = os.path.join(pdf_folder, \"merged_mlk_records.pdf\")\n",
    "    output_searchable_pdf = os.path.join(pdf_folder, \"searchable_mlk_records.pdf\")\n",
    "\n",
    "    ocr_pdf_file(input_merged_pdf, output_searchable_pdf, language='eng')\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"OCR script finished. Check your 'mlk_pdfs' folder for the searchable PDF.\")\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d626fb7-1025-4a03-a3cb-d8b1e5ed2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae7a5a-db14-47ab-93d3-53eb5737c130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
